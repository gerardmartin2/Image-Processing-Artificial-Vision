{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"kHOhUO2c3sfL"},"source":["# **Image Processing and Computer Vision (PIVA)**\n","2022 - Javier Ruiz Hidalgo - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group // [ETSETB â€“ UPC.TelecosBCN](https://telecos.upc.edu/ca)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B-V9rF2-WP4_"},"source":["# Lab - Instance Segmentation\n","The goal of this laboratory is to finetune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https://www.cis.upenn.edu/~jshi/ped_html/). We will evaluate the model before and after the fine-tunning to see the improvements on an instance segmentation task.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bX0rqK-A3Nbl"},"source":["## 1. Penn-Fudan dataset\n","\n","We will use this dataset to detect and segment people. \n","\n","First, let's download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:43:44.991746Z","iopub.status.busy":"2023-05-30T08:43:44.991316Z","iopub.status.idle":"2023-05-30T08:44:04.311661Z","shell.execute_reply":"2023-05-30T08:44:04.310442Z","shell.execute_reply.started":"2023-05-30T08:43:44.991700Z"},"id":"_t4TBwhHTdkd","trusted":true},"outputs":[],"source":["# download the Penn-Fudan dataset\n","!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n","# extract it in the current folder\n","!unzip -q PennFudanPed.zip\n","!pip install opencv-contrib-python==4.7.0.72"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WfwuU-jI3j93"},"source":["Let's have a look at the dataset and how it is layed down.\n","\n","The data is structured as follows\n","```\n","PennFudanPed/\n","  PedMasks/\n","    FudanPed00001_mask.png\n","    FudanPed00002_mask.png\n","    FudanPed00003_mask.png\n","    FudanPed00004_mask.png\n","    ...\n","  PNGImages/\n","    FudanPed00001.png\n","    FudanPed00002.png\n","    FudanPed00003.png\n","    FudanPed00004.png\n","```\n","\n","Here is one example of an image in the dataset, with its corresponding instance segmentation mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:44:04.314969Z","iopub.status.busy":"2023-05-30T08:44:04.314215Z","iopub.status.idle":"2023-05-30T08:44:04.390541Z","shell.execute_reply":"2023-05-30T08:44:04.389708Z","shell.execute_reply.started":"2023-05-30T08:44:04.314926Z"},"id":"LDjuVFgexFfh","trusted":true},"outputs":[],"source":["from PIL import Image\n","Image.open('PennFudanPed/PNGImages/FudanPed00012.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T21:48:41.961772Z","iopub.status.busy":"2023-05-29T21:48:41.961374Z","iopub.status.idle":"2023-05-29T21:48:41.973993Z","shell.execute_reply":"2023-05-29T21:48:41.972791Z","shell.execute_reply.started":"2023-05-29T21:48:41.961743Z"},"id":"cFHKCvCTxiff","trusted":true},"outputs":[],"source":["mask = Image.open('PennFudanPed/PedMasks/FudanPed00012_mask.png').convert('P')\n","# each mask instance has a different color, from zero to N, where\n","# N is the number of instances. In order to make visualization easier,\n","# let's adda color palette to the mask.\n","mask.putpalette([\n","    0, 0, 0, # black background\n","    255, 0, 0, # index 1 is red\n","    255, 255, 0, # index 2 is yellow\n","    255, 153, 0, # index 3 is orange\n","])\n","mask"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PJqSbTUU-SDz"},"source":["## 2. Training and evaluation functions\n","In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n","Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n","\n","Let's copy those files (and their dependencies) in here so that they are available in the notebook\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:28:27.126860Z","iopub.status.busy":"2023-05-30T08:28:27.126032Z","iopub.status.idle":"2023-05-30T08:29:44.240015Z","shell.execute_reply":"2023-05-30T08:29:44.238520Z","shell.execute_reply.started":"2023-05-30T08:28:27.126796Z"},"id":"cLOYjNxI-bPy","trusted":true},"outputs":[],"source":["# Download TorchVision repo to use some files from\n","# references/detection\n","!git clone https://github.com/pytorch/vision.git\n","!cd vision\n","!git checkout v0.15.1\n","\n","!cp /kaggle/working/vision/references/detection/utils.py /kaggle/working\n","!cp /kaggle/working/vision/references/detection/transforms.py /kaggle/working\n","!cp /kaggle/working/vision/references/detection/coco_eval.py /kaggle/working\n","!cp /kaggle/working/vision/references/detection/engine.py /kaggle/working\n","!cp /kaggle/working/vision/references/detection/coco_utils.py /kaggle/working"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aGD1AMGn-fV-"},"source":["Let's write some helper functions for data augmentation / transformation, which leverages the functions in `refereces/detection` that we have just copied:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:30:29.305071Z","iopub.status.busy":"2023-05-30T08:30:29.304622Z","iopub.status.idle":"2023-05-30T08:31:02.486017Z","shell.execute_reply":"2023-05-30T08:31:02.484891Z","shell.execute_reply.started":"2023-05-30T08:30:29.305032Z"},"trusted":true},"outputs":[],"source":["!pip install pycocotools"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:31:06.169675Z","iopub.status.busy":"2023-05-30T08:31:06.169313Z","iopub.status.idle":"2023-05-30T08:31:06.176276Z","shell.execute_reply":"2023-05-30T08:31:06.174923Z","shell.execute_reply.started":"2023-05-30T08:31:06.169644Z"},"id":"0yEz1jU4-kFW","trusted":true},"outputs":[],"source":["from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.PILToTensor())\n","    transforms.append(T.ConvertImageDtype(torch.float))\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kIK0TKYp_Lps"},"source":["## 3. Writing a custom dataset for Penn-Fudan\n","\n","Let's write a dataset for the Penn-Fudan dataset. Each image has a corresponding segmentation mask, where each color correspond to a different instance. We will write a `torch.utils.data.Dataset` class for this dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:44:20.586471Z","iopub.status.busy":"2023-05-30T08:44:20.586109Z","iopub.status.idle":"2023-05-30T08:44:20.601594Z","shell.execute_reply":"2023-05-30T08:44:20.600576Z","shell.execute_reply.started":"2023-05-30T08:44:20.586438Z"},"id":"mTgWtixZTs3X","trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","\n","import torch\n","import torch.utils.data\n","\n","from torchvision import transforms\n","from torchvision import utils as tutils\n","\n","\n","import skimage.transform as sktf\n","import skimage.io as skio\n","\n","from PIL import Image\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:31:10.855204Z","iopub.status.busy":"2023-05-30T08:31:10.854796Z","iopub.status.idle":"2023-05-30T08:31:11.884304Z","shell.execute_reply":"2023-05-30T08:31:11.883388Z","shell.execute_reply.started":"2023-05-30T08:31:10.855172Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","\n","import torch\n","import torch.utils.data\n","\n","from torchvision import transforms\n","from torchvision import utils as tutils\n","\n","\n","import skimage.transform as sktf\n","import skimage.io as skio\n","\n","from PIL import Image\n","\n","\n","class DatasetPerson(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:01.978313Z","iopub.status.busy":"2023-05-30T08:33:01.977936Z","iopub.status.idle":"2023-05-30T08:33:02.907158Z","shell.execute_reply":"2023-05-30T08:33:02.906179Z","shell.execute_reply.started":"2023-05-30T08:33:01.978284Z"},"trusted":true},"outputs":[],"source":["dataset = DatasetPerson('/kaggle/input/personespiva/dataset_person/data/',get_transform(train=False))\n","(img,target) = dataset[11]\n","\n","# Let's see the shape of the tensors\n","print(img.shape)\n","print(target[\"masks\"].shape)\n","print(target[\"labels\"].shape)\n","print(target[\"boxes\"].shape)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"J6f3ZOTJ4Km9"},"source":["That's all for the dataset. Let's see how the outputs are structured for this dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:44:23.862929Z","iopub.status.busy":"2023-05-30T08:44:23.862544Z","iopub.status.idle":"2023-05-30T08:44:23.885865Z","shell.execute_reply":"2023-05-30T08:44:23.884890Z","shell.execute_reply.started":"2023-05-30T08:44:23.862898Z"},"id":"ZEARO4B_ye0s","trusted":true},"outputs":[],"source":["dataset = PennFudanDataset('PennFudanPed/',get_transform(train=False))\n","(img,target) = dataset[11]\n","\n","# Let's see the shape of the tensors\n","print(img.shape)\n","print(target[\"masks\"].shape)\n","print(target[\"labels\"].shape)\n","print(target[\"boxes\"].shape)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eiovrqPjBSXn"},"source":["And see an example again on the masks and bounding boxes on top of the image. We will use a helper funcion to draw the masks and detections on top of the images:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:44:26.243486Z","iopub.status.busy":"2023-05-30T08:44:26.242800Z","iopub.status.idle":"2023-05-30T08:44:26.260067Z","shell.execute_reply":"2023-05-30T08:44:26.258886Z","shell.execute_reply.started":"2023-05-30T08:44:26.243451Z"},"id":"4MT772heZ1J7","trusted":true},"outputs":[],"source":["COCO_NAMES = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","COLORS = np.random.uniform(0, 255, size=(len(COCO_NAMES), 3)).astype(int)\n","import cv2\n","import random\n","\n","def draw_segmentation_map(image, target, score_thres=0.8):\n","    \n","    # Convert back to numpy arrays\n","    _image = np.copy(image.cpu().detach().numpy().transpose(1,2,0)*255)\n","    _masks = np.copy(target['masks'].cpu().detach().numpy().astype(np.float32))\n","    _boxes = np.copy(target['boxes'].cpu().detach().numpy().astype(int))\n","    _labels = np.copy(target['labels'].cpu().detach().numpy().astype(int))\n","    if \"scores\" in target:\n","      _scores = np.copy(target[\"scores\"].cpu().detach().numpy())\n","    else:\n","      _scores = np.ones(len(_masks),dtype=np.float32)\n","\n","    alpha = 0.3\n","    \n","    label_names = [COCO_NAMES[i] for i in _labels]\n","\n","    # Add mask if _scores\n","    m = np.zeros_like(_masks[0].squeeze())\n","    for i in range(len(_masks)):\n","      if _scores[i] > score_thres:\n","        m = m + _masks[i]\n","    \n","    # Make sure m is the right shape\n","    m = m.squeeze()\n","\n","    # dark pixel outside masks\n","    _image[m<0.5] = 0.3*_image[m<0.5]\n","\n","    # convert from RGB to OpenCV BGR and back (cv2.rectangle is just too picky)\n","    _image = cv2.cvtColor(_image, cv2.COLOR_RGB2BGR)\n","    _image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n","\n","    for i in range(len(_masks)):\n","      if _scores[i] > score_thres:         \n","        # apply a randon color to each object\n","        color = COLORS[random.randrange(0, len(COLORS))].tolist()\n","                \n","        # draw the bounding boxes around the objects\n","        cv2.rectangle(_image, _boxes[i][0:2], _boxes[i][2:4], color=color, thickness=2)\n","        # put the label text above the objects\n","        cv2.putText(_image , label_names[i], (_boxes[i][0], _boxes[i][1]-10), \n","                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, \n","                    thickness=1, lineType=cv2.LINE_AA)\n","    \n","    return _image/255"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:09.196930Z","iopub.status.busy":"2023-05-30T08:33:09.196558Z","iopub.status.idle":"2023-05-30T08:33:10.850859Z","shell.execute_reply":"2023-05-30T08:33:10.850052Z","shell.execute_reply.started":"2023-05-30T08:33:09.196900Z"},"id":"hKiP2DziLjUR","trusted":true},"outputs":[],"source":["import plotly.express as px\n","px.imshow(draw_segmentation_map(img, target))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jR8_0CQ6W4cW"},"source":["## 4. Using a pre-trained Mask R-CNN model\n","\n","We will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image. Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:31:27.575272Z","iopub.status.busy":"2023-05-30T08:31:27.574912Z","iopub.status.idle":"2023-05-30T08:31:29.147277Z","shell.execute_reply":"2023-05-30T08:31:29.146283Z","shell.execute_reply.started":"2023-05-30T08:31:27.575241Z"},"id":"bYJ3gGynXJT9","trusted":true},"outputs":[],"source":["import torchvision \n","\n","model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT', progress=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lTpWHEPbYQ2V"},"source":["Set the model to GPU and evaluation mode:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:31:34.075300Z","iopub.status.busy":"2023-05-30T08:31:34.074947Z","iopub.status.idle":"2023-05-30T08:31:37.087444Z","shell.execute_reply":"2023-05-30T08:31:37.086404Z","shell.execute_reply.started":"2023-05-30T08:31:34.075272Z"},"id":"HOQMsE7xYA3W","trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using {device}.\")\n","model.to(device).eval()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k7K5lPR8RmJO"},"source":["Let us show the prediction of the pre-trained model, as a reminder we use this input image with the following groundtruth:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:16.930993Z","iopub.status.busy":"2023-05-30T08:33:16.930609Z","iopub.status.idle":"2023-05-30T08:33:17.378552Z","shell.execute_reply":"2023-05-30T08:33:17.375909Z","shell.execute_reply.started":"2023-05-30T08:33:16.930959Z"},"id":"avy7XpimS5R4","trusted":true},"outputs":[],"source":["from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","\n","fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Input\", \"Ground Truth\"))\n","fig.add_trace(go.Image(z=img.numpy().transpose(1,2,0)*255), 1, 1)\n","fig.add_trace(go.Image(z=draw_segmentation_map(img, target)*255), 1, 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DNYMZh88IDgF"},"source":["Pass the image (as a batch) through the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:24.481370Z","iopub.status.busy":"2023-05-30T08:33:24.481011Z","iopub.status.idle":"2023-05-30T08:33:29.127395Z","shell.execute_reply":"2023-05-30T08:33:29.126411Z","shell.execute_reply.started":"2023-05-30T08:33:24.481341Z"},"id":"ohx3APJJYWLM","trusted":true},"outputs":[],"source":["# add a batch dimension\n","imgs = img.unsqueeze(0).to(device) #torch.stack((img,img))\n","outs = model(imgs.to(device))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kYVB9IQ4HCci"},"source":["Let's check how many predictions, labels and scores are found for this image (see the COCO_NAMES list for the correspondence between label numbers and semantic meaning):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:32.752664Z","iopub.status.busy":"2023-05-30T08:33:32.752301Z","iopub.status.idle":"2023-05-30T08:33:32.760179Z","shell.execute_reply":"2023-05-30T08:33:32.758440Z","shell.execute_reply.started":"2023-05-30T08:33:32.752631Z"},"id":"JNnUD5auDFSP","trusted":true},"outputs":[],"source":["print(f\"Number of predictions = {len(outs[0]['labels'])}\")\n","print(f\"  labels = {outs[0]['labels'].cpu().numpy()}\")\n","print(f\"  scores = {outs[0]['scores'].detach().cpu().numpy()}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OaA0l_msIQx3"},"source":["Let's show the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:05:22.356591Z","iopub.status.busy":"2023-05-30T06:05:22.356228Z","iopub.status.idle":"2023-05-30T06:05:22.891689Z","shell.execute_reply":"2023-05-30T06:05:22.888538Z","shell.execute_reply.started":"2023-05-30T06:05:22.356562Z"},"id":"PHSAv0WIIPef","trusted":true},"outputs":[],"source":["fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Prediction (all scores)\", \"Prediction (scores>0.8)\"))\n","fig.add_trace(go.Image(z=draw_segmentation_map(img, outs[0], score_thres=0.0)*255), 1, 1)\n","fig.add_trace(go.Image(z=draw_segmentation_map(img, outs[0], score_thres=0.8)*255), 1, 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AjxR4DNpKOUL"},"source":["## 5. Testing the pre-trained Mask R-CNN model\n","\n","First, we separate the datasets into training (every image except the last 50) and test (last 50 images):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:33:35.602558Z","iopub.status.busy":"2023-05-30T08:33:35.601877Z","iopub.status.idle":"2023-05-30T08:33:35.636473Z","shell.execute_reply":"2023-05-30T08:33:35.635404Z","shell.execute_reply.started":"2023-05-30T08:33:35.602520Z"},"id":"ftM0WT3Iy9qB","trusted":true},"outputs":[],"source":["# use our dataset and defined transformations\n","dataset = DatasetPerson('/kaggle/input/personespiva/dataset_person/data/',get_transform(train=False))\n","dataset_test = DatasetPerson('/kaggle/input/personespiva/dataset_person/data/',get_transform(train=False))\n","\n","# split the dataset in train and test set (30%)\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-1500])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-1500:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n","    collate_fn=utils.collate_fn)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"peEZNprdMhJC"},"source":["We will evaluate the test:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T22:54:02.468448Z","iopub.status.busy":"2023-05-29T22:54:02.468080Z","iopub.status.idle":"2023-05-29T22:54:47.624006Z","shell.execute_reply":"2023-05-29T22:54:47.621678Z","shell.execute_reply.started":"2023-05-29T22:54:02.468419Z"},"id":"6UUysIxrM51C"},"outputs":[],"source":["evaluate(model, data_loader_test, device=device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"d1VckiefNmzr"},"source":["We can focus on the value of the Average Precision for IoU=0.50:0.95 (first row as it can be seen as an average of the rest of the rows) with a value of 0.843 for bounding boxes or 0.734 for masks (quite good already!)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OW49GFrgN1ha"},"source":["## 6. Train Mask-RCNN on the Penn-Fudan dataset\n","\n","We can change the classifier of the Mask-RCNN to train it with this dataset and compare the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:32:04.272943Z","iopub.status.busy":"2023-05-30T08:32:04.272565Z","iopub.status.idle":"2023-05-30T08:32:05.226239Z","shell.execute_reply":"2023-05-30T08:32:05.225172Z","shell.execute_reply.started":"2023-05-30T08:32:04.272911Z"},"id":"P8td9sEn0aQG","trusted":true},"outputs":[],"source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","num_classes = 2\n","\n","# load an instance segmentation model pre-trained on COCO\n","model2 = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT', progress=True)\n","\n","# get the number of input features for the classifier\n","in_features = model2.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model2.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","# now get the number of input features for the mask classifier\n","in_features_mask = model2.roi_heads.mask_predictor.conv5_mask.in_channels\n","hidden_layer = 256\n","# and replace the mask predictor with a new one\n","model2.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                    hidden_layer,\n","                                                    num_classes)\n","\n","model2.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:32:09.919512Z","iopub.status.busy":"2023-05-30T08:32:09.919150Z","iopub.status.idle":"2023-05-30T08:32:09.928685Z","shell.execute_reply":"2023-05-30T08:32:09.927546Z","shell.execute_reply.started":"2023-05-30T08:32:09.919481Z"},"id":"vqRFB6eU0C5T","trusted":true},"outputs":[],"source":["# construct an optimizer\n","params = [p for p in model2.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:05:43.078312Z","iopub.status.busy":"2023-05-30T06:05:43.077982Z","iopub.status.idle":"2023-05-30T06:05:47.858777Z","shell.execute_reply":"2023-05-30T06:05:47.857355Z","shell.execute_reply.started":"2023-05-30T06:05:43.078285Z"},"id":"iMx-vfDR046S","trusted":true},"outputs":[],"source":["# let's train it for 20 epochs\n","from torch.optim.lr_scheduler import StepLR\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model2, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","torch.save(model2, 'model2_train_test.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:51:47.428595Z","iopub.status.busy":"2023-05-30T08:51:47.428194Z","iopub.status.idle":"2023-05-30T08:51:49.451347Z","shell.execute_reply":"2023-05-30T08:51:49.450399Z","shell.execute_reply.started":"2023-05-30T08:51:47.428565Z"},"id":"Kbe4mBqkdVYW","trusted":true},"outputs":[],"source":["# evaluate on the test dataset\n","#load the model trained in Kaggle\n","model2 = torch.load(\"/kaggle/input/model2/model2.pth\")\n","#evaluate(model2, data_loader_test, device=device)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"of1m1b93So52"},"source":["And we can compare the test results with the pre-trained model. As we have a small dataset with not much variability, the small training helps achieving slightly better results."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:51:51.937669Z","iopub.status.busy":"2023-05-30T08:51:51.937312Z","iopub.status.idle":"2023-05-30T08:51:52.045019Z","shell.execute_reply":"2023-05-30T08:51:52.043745Z","shell.execute_reply.started":"2023-05-30T08:51:51.937640Z"},"id":"rQ7EtFfIZWRL","trusted":true},"outputs":[],"source":["(img,target) = dataset_test[6]\n","imgs = img.unsqueeze(0).to(device) #torch.stack((img,img))\n","outs = model2(imgs)\n","\n","fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Prediction (all scores)\", \"Prediction (scores>0.8)\"))\n","fig.add_trace(go.Image(z=draw_segmentation_map(img, outs[0], score_thres=0.0)*255), 1, 1)\n","fig.add_trace(go.Image(z=draw_segmentation_map(img, outs[0], score_thres=0.8)*255), 1, 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aV5amPtmacgb"},"source":["Congratulations you have finished the lab session!\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FAmuq4HPLM4e"},"source":["## 6. 1st PIVA Person Segmentation Challenge (PPSC-2023)\n","\n","In this lab, we are going to ask you to enter the **1st PIVA Person Segmentation Challenge (PPSC-2023)**. We are going to provide you with a [dataset](https://drive.google.com/file/d/1zChno9LQBV8rpxRTOes4W6Kg156KsCTY/view?usp=sharing) of images (with groundtruth) and a set of test images (without groundtruth). You will have to generate a model and send the test results to the challenge in order to evaluate your performance. \n","\n","You will have to submit:\n","\n","1. A report as a PDF file (no more than 3 pages) with an explanation of what you did to obtain, train and test the model (see further comments).\n","2. A .zip file with the predictions (as images, see further comments) of all test images.\n","3. A notebook python file with your model, training, pre- and post- processing code.\n","4. Submit all files (separately, not in another zip file) in the submission task in Atenea.\n","\n","The rules of the competition are:\n","\n","1. You can use any architecture you want. You can use the one in this lab session, a modified version of it or something different. But always explain in the report your selection.\n","2. Explain any training, fine-tunning or hyper-parameter search you do to your selected model.\n","3. Explain all datasets used by your model (with the splits performed).\n","4. Explain any pre-processing/post-processing you do to the images/predictions.\n","5. Explain the losses you use to train or fine-tune your system.\n","6. As most of the images in the challenge dataset are ony of a single person, the metric to evaluate the results will be the mean of the global IoU of all images in the test split. Comment in your report the advantages/disadvantages of using this metric in the test evaluation.\n","\n","The prediction format should be:\n","\n","1. A zip file with a number of PNG files, one for each test image (same filename as the test images but with `.png` extension).\n","2. The PNG images must be grayscale images with a value of `0` for the backgrund and `>0` for person masks. As IoU is employed as metric, the prediction should have only objects of the class person and all pixels `>0` will be considered person mask.\n","4. There is a example code to generate a fake submission `.zip` with a made-up segmentation.\n","5. There is also an example code to check your submission format. It is **mandatory** to use it, Make sure that your zip file passes it without errors or warnings before your submission.\n","6. After all submission are received a leaderboard will be published.\n","\n","Good luck!\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["El model de dalt (model2), entrenat amb 20 Ã¨poques i el dataset de Persones Ã©s el que hem usat per fer l'estudi, per tant no l'hem tornat a copiar baix."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:34:16.342727Z","iopub.status.busy":"2023-05-30T08:34:16.341102Z","iopub.status.idle":"2023-05-30T08:34:16.348890Z","shell.execute_reply":"2023-05-30T08:34:16.347861Z","shell.execute_reply.started":"2023-05-30T08:34:16.342677Z"},"id":"tcfqMx0-2XdP","trusted":true},"outputs":[],"source":["# Make sure to change this for the path of the \"dataset_person/test\" folder and your group/team name\n","ZIPFILE = 'G12_E12'\n","TEST_FOLDER = \"/kaggle/input/personespiva/dataset_person/test/\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T08:41:31.875973Z","iopub.status.busy":"2023-05-30T08:41:31.874952Z","iopub.status.idle":"2023-05-30T08:42:47.157085Z","shell.execute_reply":"2023-05-30T08:42:47.155992Z","shell.execute_reply.started":"2023-05-30T08:41:31.875915Z"},"trusted":true},"outputs":[],"source":["# Let's create fake predictions for the test images\n","from PIL import Image\n","import shutil\n","import random\n","import os\n","import plotly.graph_objects as go\n","\n","import torch\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Use a seed to control random generation\n","r = random.Random(5)\n","\n","# Create the directory\n","out_path= os.path.join('/kaggle/working/','MASKS_PREDICTION')\n","os.makedirs(out_path, exist_ok=True)\n","\n","score_threshold = 0.75\n","\n","for f in open(os.path.join(TEST_FOLDER,'test_names.txt'),'r'):\n","    b = f.strip()\n","    # read test image\n","    img = Image.open(os.path.join(TEST_FOLDER,'images',b)+'.jpg')\n","    \n","    transform = transforms.ToTensor()\n","    img = transform(img)\n","    \n","    img = img.unsqueeze(0).to(device) #torch.stack((img,img))\n","    outs = model(img)\n","    \n","    scores = np.copy(outs[0][\"scores\"].cpu().detach().numpy())\n","    mask = np.copy(outs[0][\"masks\"].cpu().detach().numpy())\n","    \n","    wdth, hght = mask.shape[-1], mask.shape[-2]\n","    \n","    # Add mask if scores\n","    m = torch.zeros((hght, wdth)).cpu().numpy()\n","    \n","    for i in range(len(mask)):\n","        if scores[i] > score_threshold:\n","            m = m + mask[i]\n","            \n","    preds = m.squeeze()\n","    min_p, max_p = preds.min(), preds.max()\n","    \n","    preds = ((preds - min_p) / (max_p - min_p) * 255).astype(np.uint8)\n","    pred = Image.fromarray(preds, mode='L') \n","    \n","    pred.save(os.path.join(str(out_path),b)+'.png')\n","# zip all predictions\n","shutil.make_archive(ZIPFILE, 'zip', out_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T07:06:04.405492Z","iopub.status.busy":"2023-05-30T07:06:04.405118Z","iopub.status.idle":"2023-05-30T07:06:06.618072Z","shell.execute_reply":"2023-05-30T07:06:06.617112Z","shell.execute_reply.started":"2023-05-30T07:06:04.405457Z"},"id":"Juzcsuxz2zho","trusted":true},"outputs":[],"source":["# Sample script to compute mean IoU of predictions in the TEST. MANDATORY to check this on your file before the submission\n","TEST_FOLDER2 = \"/kaggle/working/\"\n","# unzip the file into a tmp folder\n","os.makedirs(os.path.join(TEST_FOLDER2,'tmp3'))\n","shutil.unpack_archive(ZIPFILE + '.zip',os.path.join(TEST_FOLDER2,'tmp3'))\n","\n","# Use a seed to control random generation\n","r = random.Random(7)\n","\n","mIoU = 0.0\n","\n","for cnt,f in enumerate(open(os.path.join(TEST_FOLDER,'test_names.txt'),'r')):\n","\n","    b = f.strip()\n","\n","    # read test image (we do not needed it but here we will use it to get the size of GT)\n","    im = Image.open(os.path.join(TEST_FOLDER,'images',b)+'.jpg')\n","\n","    # read GT image (as a test we can generate the fake predictions)\n","    #gt = np.asarray(Image.open(os.path.join(TEST_FOLDER,'masks',b)+'.png'))\n","    gt = np.zeros((im.height,im.width),np.uint8)\n","    for o in range(r.randint(1,3)):\n","        sx = r.randint(im.width//8,im.width//2)\n","        x = r.randint(0,im.width-sx)\n","        sy = r.randint(im.height//8,im.height//2)\n","        y = r.randint(0,im.height-sy)\n","        gt[y:y+sy,x:x+sx] = o+1\n","    gtb = (gt>0) # we'll consider a global IoU for all objects\n","\n","    # read prediction image\n","    pred = np.asarray(Image.open(os.path.join(TEST_FOLDER2,'tmp3',b)+'.png'))\n","    predb = (pred>0) # we'll consider a global IoU for all objects\n","    \n","    # Compute the global IoU in the image\n","    overlap = gtb * predb\n","    union   = gtb + predb    \n","    IoU = overlap.sum()/union.sum()\n","        \n","    mIoU += IoU\n","\n","# Report results\n","# mIoU = mIoU / (cnt+1)\n","# print(f\"mIoU = {mIoU:0.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":4}
